\documentclass[12pt,letterpaper]{article}
\usepackage{latexsym}
\usepackage[english, activeacute]{babel}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{enumerate}
\usepackage{
	epsfig}
\linespread{1.3}
\parskip 1ex  \parindent 0ex
%\voffset -15mm
\oddsidemargin -0mm \topmargin 0mm \headheight 0pt \headsep 0pt
\textwidth 6.5in \textheight 9in %\marginparsep 0pt \marginparwidth 0pt
\renewcommand{\baselinestretch}{1.1}
\newtheorem{prop}{Proposition}
\newtheorem{Cor}{Corollary}
\newtheorem{lema}{Lemma}
\usepackage{color}

\renewcommand\a{{\bf a}}
\renewcommand\b{{\bf b}}
\newcommand\e{{\bf e}}
\newcommand\g{{\bf g}}
\newcommand\x{{\bf x}}
\renewcommand\d{{\bf d}}
\newcommand\bz{{\bf 0}}

\begin{document}
\pagestyle{empty}
\def \NATUR{ I \hspace*{-0.8ex} N}
\def \REALES{ I \hspace*{-0.8ex} R}
\renewcommand{\labelenumi}{\alph{enumi})}
\renewcommand{\labelenumii}{\roman{enumii})}

\hspace{-6mm}
\begin{tabular}{lcr}
CME 307 / MS\&E 311 & \hspace{3in} & Winter 2019-2020 \\ Optimization & & Feb 19, 2020 \\
Prof. Yinyu Ye & & Due Friday Mar 13 11:00am
\end{tabular}

\bigskip

\begin{center}
{\large \bf Homework Assignment 3 \\  Discuss Session Friday Mar 6 in Class}
\end{center}
%\vfill

%{}

\bigskip

{\textbf{Reading.}} Read selected sections in Luenberger and Ye's {\sl Linear
and Nonlinear Programming Fourth Edition} Chapters 3, 5, 6, 8, 9, 10, and 14.

{\textbf{Individual Homework (70'):}}
\emph{Please submit your answer to Gradescope ``HW3 - Individual''. We strongly encourage students to form study groups. Students may discuss and work on homework problems in groups. However, each student must write down the solutions of the following questions \textbf{independently}, and without referring to written notes from the joint session. In other words, each student must understand the solution well enough in order to reconstruct it by him/herself. In addition, each student should write on the problem set the set of people with whom s/he collaborated.}

\begin{enumerate}

\item[1.] (10') In most real applications, the (first-order) Lipschitz constant $\beta$ is unknown. Furthermore, we like to use a localized Lipschitz constant $\beta^k$ at iteration $k$ such that
\[
	f(\x^k + \alpha \d^k) - f(\x^k) - \nabla f(\x^k)^T(\alpha \d^k) \leq \frac{\beta^k}{2} \|\alpha \d^k\|^2,
\]
where $\d^k$ is the steepest descent direction $-\nabla f(\x^k)$. The goal is to decide a step-size $\alpha \approx \frac{1}{\beta^k}$.

Consider the following \emph{forward-backward tracking method}. In the following, assume that $\beta^k\geq 1$ and $\alpha_{\max}\geq 1/\beta^k$. Notice that if $\beta^k<1$, we can enforce it to satisfy our assumption by replacing it with $\max\{1,\beta^k\}$. 

Now start at a initial guess $\alpha > 0$,
\begin{enumerate}
	\item If $\alpha \leq \frac{2 (f(\x^k) - f(\x^k + \alpha \d^k))}{\|\d^k\|^2}$, then doubling the step-size: $\alpha \gets 2\alpha$, stop as soon as the inequality is reversed or $\alpha>\alpha_{\max}(>0)$, and select the latest $\alpha$ such that the inequality ($\alpha \leq \frac{2 (f(\x^k) - f(\x^k + \alpha \d^k))}{\|\d^k\|^2}$) holds and $\alpha\leq \alpha_{\max}$.
	\item Otherwise halving the step-size: $\alpha \gets \alpha/2$; stop as soon as $\alpha \leq \frac{2 (f(\x^k) - f(\x^k + \alpha \d^k))}{\|\d^k\|^2}$ and return it.
\end{enumerate}

\begin{enumerate}
	\item [(a)] (4') Let $\bar{\alpha}$ be a step-size generated by the scheme. Show that $\bar{\alpha} \geq \frac{1}{2\beta^k}$.
	\item [(b)] (3') Prove that the above scheme will terminate in finite steps.
	\item [(c)] (3') Show that $f(\x^k + \bar{\alpha} \d^k) \leq f(\x^k) - \frac{1}{4\beta^k} \|\d^k\|_2^2$.
\end{enumerate}


\item[2.] (10') ($L_2$ Regularization and Logarithmic Barrier) Consider the optimization problem
\begin{align*}
\mathrm{minimize}_{x_1, x_2}   \qquad &  (x_1-x_2+1)^2\\
\mathrm{subject~to} \qquad & x_1\ge 0 \quad x_2\ \text{``free''} .
\end{align*}
Then we may combine the $L_2$-regularization and barrier together, that is, for any $\mu>0$, consider
\[
    \mathrm{minimize}_{x_1, x_2} \qquad  (x_1-x_2+1)^2+\frac{\mu}{2}(x_1^2+x_2^2)-\mu\log(x_1)  
\]

\begin{enumerate}[(a)]
\item (4') Develop explicit path formula in terms of $\mu$. What is the limit solution as $\mu\rightarrow 0$?

\item (3') Using $\mu=1$ and $\x^0=(1,\ 0)$, apply one step of SDM with step-size $1/5$ to compute the next iterate.

\item (3') Using $\mu=1$ and $\x^0=(1,\ 0)$, apply one step of Newton's Method to compute the next iterate.
\end{enumerate}

\item[3.] (20') ($L_2$ Path-Following) Consider a convex function $f: R^n \to R$ in $C^2$ that is twice continuously differentiable.  Assume that its value is bounded from below and that it has a minimizer.  For any given positive parameter $\mu>0$, consider the regulated minimization problem
\begin{equation}\label{path}
\text{minimize}\ f(\x)+\frac{\mu}{2}\|\x\|^2.
\end{equation}

\begin{enumerate}
\item[(a)] (3') Write down the first-order optimality condition of (\ref{path}). Is it sufficient for $\x$ to be a minimizer?

\item[(b)] (3') Show that the minimizer, denoted by $\x(\mu)$, of (\ref{path}) is unique for each fixed $\mu$.

\item[(c)] (5') Show that $f(\x(\mu))$ is an increasing function of $\mu$ (i.e., $\ f(\x(\mu))\ge f(\x(\mu'))\ $ if $\mu\ge\mu'>0\ $), and $\|\x(\mu)\|$ is a decreasing function of $\mu$.

\item[(d)] (6') Show that As $\mu\rightarrow 0^+$ (i.e., $\mu$ decreases to $0$\ ), $\x(\mu)$ converges to the minimizer of $f(\x)$ with the minimal Euclidean norm.

\item[(e)] (3') Consider the specific example 
\[\text{minimize}_{x_1,x_2}\ (x_1-x_2-1)^2,\]
where the optimal solution set is unbounded. Write out the explicit path formula of $\x(\mu)=(x_1(\mu),\dots,x_n(\mu))$ in terms of $\mu$. What is the limit solution as $\mu\rightarrow 0$?
\end{enumerate}
\clearpage

\item[4.] (15') (Gradient-Projection) Consider the following conic constrained optimization problem
\begin{equation}
	\min_{x} \quad f(\x) \qquad \mathrm{s.t.} \quad \x \geq \bz
	\label{eq:conic}
\end{equation}

Consider the following gradient-projection method: at a given point $\x^k = (x^k_1, x^k_2, \dots, x^k_n)\ge \bz$, 
take $\x^{k+1} = \mathrm{Proj}_{\mathbb{R}^n_+} \left(\x^k- \frac{1}{\beta} \nabla f(\x^k) \right)$,
namely for all $i = 1,\ldots, n$,
\[
	x_i^{k+1}=\max \left\{0, x_i^k- \frac{1}{\beta} \left(\nabla f(\x^k) \right)_i \right\},
\]
where $\beta$ is the (first-order) Lipschitz constant.

\begin{enumerate}
\item [(a)] (3') Write down the KKT conditions for \eqref{eq:conic}.
\item [(b)] (3') Show that if $\x^{k+1} = \x^k$ then $\x^k$ satisfies the first-order KKT conditions.
\item [(c)] (6') Show that $f(\x^{k+1}) - f(\x^k) \leq - \frac{\beta}{2} \|\x^{k+1} - \x^k\|^2$.
\item [(d)] (3') Derive a iterative complexity bound for $\|\x^{k+1} - \x^k\| \leq \varepsilon$.
\end{enumerate}

\item[5.] (15') (Affine-Scaling Interior-Point SD) 
Consider the conic constrained optimization problem \eqref{eq:conic} again, where we assume the objective function $f$ is first-order $\beta$-Lipschitz.
Starting from $\x^0=\e>0$, consider the affine-scaling interior-point method as follows: at iterate $\x^k>0$ let diagonal scaling matrix $D$ be
\[D_{ii}=\min\{1,\ x^k_i\}\]
and
\[\x^{k+1}=\x^k-\alpha^kD^2\nabla f(\x^k),\]
with step-size
\[\alpha^k=\min \left\{\frac{1}{\beta},\ \frac{1}{2\|D\nabla f(\x^k)\|_{\infty}} \right\}.\]
\begin{enumerate}
\item[(a)] (3') Show that $-D^2 \nabla f(\x^k)$ is a descent direction.

\item[(b)] (3') Show that $\x^{k+1}>\bz$ for all $k=0,1,...$.

\item[(c)] (6') Show that 
\[
	f(\x^{k+1}) - f(\x^k) \leq -\min \left\{ \frac{1}{2\beta} \|D \nabla f(\x^k)\|_\infty^2, \frac{1}{4} \| D \nabla f(\x^k) \|_\infty \right\}	
\] 
\item[(d)] (3') Derive a iterative complexity bound for $\|D\nabla f(\x^k)\|_{\infty}\le \epsilon$.
\end{enumerate}

\clearpage

\end{enumerate}

\clearpage
\textbf{Computational Groupwork (70')  (group of 1-4 people)}:
\emph{Please submit your answer and code \textbf{in group} to Gradescope ``HW3 - Groupwork''.}

\begin{enumerate}
\item[6.] (20') Let $\{(\a_i, c_i)\}_{i=1}^m$ be a given dataset where $\a_i \in R^n$, $c_i \in \{\pm 1\}$. Consider the quadratic regularized log-logistic-loss problem
\begin{small} 
\[
\mathrm{minimize} f(\x,x_0)=\sum_{i,c_i=1}\log\left(1+\exp(-\a_i^T\x-x_0)\right)+\sum_{i,c_i=-1}\log\left(1+\exp(\a_i^T\x+x_0)\right) + 0.001 \cdot \|\x\|_2^2.
\]
\end{small}
 Consider the following data set
\[
	\a_1 = (0;0),~~ \a_2 = (1;0),~~ \a_3 = (0;1),~~ \a_4 = (0;0),~~ \a_5 = (-1;0),~~\a_6 = (0;-1),
\]
with label
\[
	c_1 = c_2 = c_3 = 1, \quad c_4 = c_5 = c_6 = -1
\]

Implement the steepest descent (SD), the accelerated SD, the BB-stepsize SD, and the BFGS methods to numerically solve this problem on the above data \textbf{and} your own classification data and compare their convergence performance.
 
\item[7.] (10') 
Consider solving $\min_{\x} \frac{1}{2}\|A\x-\b\|^2, \mathrm{s.t.} \x \geq \bz$. Generate some random data and solve them with the two methods described in Problem 4 and Problem 5. Report the results and compare their convergence.

\item[8.] (10')
There is a simple nonlinear least squares approach for Sensor Network Localization:
\begin{equation}\label{snl-nls}
\begin{array}{lll}
&\min        & \sum_{(ij)\in N_x}\left(\|\x_i-\x_j\|^2 - d_{ij}^2\right)^2+\sum_{(kj)\in N_a} \left(\|\a_k-\x_j\|^2 - d_{kj}^2\right)^2
\end{array}
\end{equation}
which is an unconstrained nonlinear minimization problem.

\begin{itemize}
\item[(a)] (5') Apply the Steepest Descent Method, starting with either the origin or a random solution as the initial solution for model (\ref{snl-nls}), to solve the SNL instances you created in Problem 9 of HW1. Does it work?

\item[(b)] (5') Apply the same Steepest Descent Method, starting from the SOCP or SDP solution (which may not have errors) as the initial solution for model (\ref{snl-nls}), to solve the same instances in (a). Does it work? Does the SOCP or SDP initial solution make a difference?
\end{itemize}

\clearpage
\item [9.] (30') (Multi-Block ADMM)
\paragraph{Part I} Implement the ADMM to solve the divergence example:
\begin{equation*}
	\begin{aligned}
		\mathrm{minimize} \quad & 0 \cdot x_1 + 0 \cdot x_2 + 0 \cdot x_3 \\
		\mathrm{subject to} \quad & 
		\begin{pmatrix}
			1 & 1 & 1 \\
			1 & 1 & 2 \\
			1 & 2 & 2 \\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 
		\end{pmatrix}
		= \bz
	\end{aligned}
\end{equation*}

\begin{enumerate}
\item[(a)] (5') Try $\beta=0.1$, $\beta=1$, and $\beta=10$, respectively. Does the choice of $\beta$ make a difference?

\item[(b)] (5') Add the objective function to minimize
\[0.5(x^2_1+x^2_2+x^2_3)\]
to the problem, and retry $\beta=0.1$, $\beta=1$, and $\beta=10$, respectively. Does the choice of $\beta$ make a difference?

\item[(c)] (5') Set $\beta=1$ and apply the randomly permuted updating-order of $\x$ (discussed in class) to solving each of the two problems in (a) and (b). Does the iterate converge?
\end{enumerate}

\paragraph{Part II} Generate some (feasible) convex QP problems with linear equality constraints, say $30$ variables and $10$ constraints (i.e., $A \in R^{10 \times 30}$),
\begin{eqnarray*}
\mbox{minimize}   & \frac{1}{2}\x^TQ\x &\\
\mbox{subject to} & A\x=\b,& \x\ge \bz.
\end{eqnarray*}

\begin{enumerate}
\item[(d)] (5') Divide the variables of $\x$ into $5$ blocks and apply the ADMM with $\beta=1$. Does it converge? (You may construct $5$ different  blocks and conduct the experiments.)

\item[(e)] (5') Apply the randomly permuted updating-order of the 5 blocks in each iteration of the ADMM. Does it converge? Convergence performance?

\item[(f)] (5') Consider the following scheme -- random-sample-without-replacement: in each iteration of ADMM, randomly sample $6$ variables for update, and then randomly select $6$ variables from the remaining $24$ variable for update, and... , till all $30$ variables are updated; then update the multipliers as usual. Does it converge? Convergence performance?
\end{enumerate}
\end{enumerate}
\end{document}
